# 零碎知识点

## Temperature
太长不看：
```
T用来调整LLM输出的不确定性, T越大, 输出结果的不确定性越高; T越小, 输出结果的不确定性越低.

注意: T为 0 时并不能认为LLM每次都会生成相同结果! 它只能让LLM输出的 "随机性" 尽可能下降.
```

Temperature 本质是上是将 LLM 原始的输出数值 (可以理解为某个词成为下一个输出的概率) 按照下式重新计算：
- 原式为：
$$
p(x_i) = \frac{e^{x_i}}{\sum_{j=1}^V e^{x_j}}
$$
- 增加 Temperature 参数 `T` 之后：
$$
p(x_i) = \frac{e^{\frac{x_i}{T}}}{\sum_{j=1}^V e^{\frac{x_i}{T}}}
$$

从这个式子很容易观察出：
 - 当 T 增加，极端情况下 T 为`无穷大`时，$\frac{x_i}{T}$ 趋近于 0, $e^{\frac{x_i}{T}}$趋近于 1, 此时的输出概率分布是一个`均匀分布`, 即词表中每一个词出现的概率均等，这会使得 LLM 输出的每一个字更具有随机性。
 - 当 T 减小，极端情况下 T 趋近于 `0` 时，$\frac{x_i}{T}$ 趋近于无穷大， $e^{\frac{x_i}{T}}$ `以更快的速度趋近于无穷大`, 这会剧烈地放大每一个词之间的概率差距，此时的概率分布虽然没有统一的概率模型去命名，但是可以感受到绘制出来的概率直方图应该是存在多个尖峰的图样，即词表中某些词将可能会频繁被 LLM 选中，使得输出结果的不确定定性更低。

## Top-K 和 Top-P
Top-K 和 Top-P 都是采样方法，主要用来限制 LLM 输出时的采样区间。

首先明确一下，LLM 每一次预测时输出的其实会首先经过 softmax 换算成概率，它本质上是一个 Tensor, 这个 Tensor 的长度等同于整个词表，描述的是词表中每一个词被选中的概率，因此，我们会将这个 Tensor 作为 LLM 的输出概率分布。
 - Top-K 会从 LLM 的输出概率分布中直接选取概率最高的 K 个 token 作为本次预测最后输出的候选 token. Top-K 的值越高，LLM 输出的随机性越高，反之越低。
   - Top-K 取值为 1 时也被称为`Greedy Decoding`
 - Top-P 的 P 相当于一个阈值，P 的取值范围是`[0,1]`; Top-P 的做法是对当前输出的 tensor 从高到低排个序，然后从最大的概率的 token 开始挑选，并将选中的 token 的概率进行累加，根据这个规则不断选取可能成为本次输出的候选 token, 直到保留下来的 token 所对应的概率之和最大且不超过 p.
   - Top-P 取值为 0 时就是 greedy decode, 取值为 1 时等同于直接选中了整个词表作为候选 token

Top-K 和 Top-P 通常需要经过实验才能确定对于当前任务的最佳取值。



# Reference
- white paper https://www.kaggle.com/whitepaper-prompt-engineering

