{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 哈希表/散列表 - Hash Table  \n",
    "* 哈希表通常是用来**快速判断一个元素是否出现集合里**。\n",
    "    * 哈希表是利用 key 直接寻找 对应 value 的一种数据结构\n",
    "        * 数组就是一张哈希表, 数组的 index 就是 value 对应的 key\n",
    "\n",
    "* 哈西表的key是根据**哈西函数**计算出来的\n",
    "\n",
    "\n",
    "### 当我们遇到了要快速判断一个元素是否出现集合里的时候，就要考虑哈希法。\n",
    "* 哈希法也是牺牲了空间换取了时间，因为我们要使用额外的数组，set或者是map来存放数据，才能实现快速的查找。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 哈西函数 -- hash function\n",
    "* 哈希函数如下图所示，通过hashCode把名字转化为数值，一般hashcode是通过特定编码方式，可以将其他数据格式转化为不同的数值，这样就把学生名字映射为哈希表上的索引数字了。\n",
    "    * 换句话说, 哈希函数是把传入的 key 转换为 符号表的索引, 比如我们使用数组作为记录数据的哈希表, 此时哈希函数的作用就是把 字母a 映射为数组的 index \n",
    "\n",
    "![jupyter](https://img-blog.csdnimg.cn/2021010423484818.png)\n",
    "\n",
    "* 转换过程:\n",
    "    * 先利用 hashCode (某种编码方法) 把各种类型的值, 比如字符串+数字等, 转换成能唯一的编码 \n",
    "    * 利用 hash table 的长度, 把 hashcode [换算] 成查表用的 index\n",
    "        * 比较简单的做法是 直接把 hash code 与表格长度取余数(用百分比计算符), 得到的结果通常就是index, 此时得到的 映射关系是 1v1 的\n",
    "    * 所以, 定义一个 hash function 就有了上图的过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哈西碰撞 -- hash collision\n",
    "* 在定义哈西函数时, 总可能出现哈希表长度太短的情况, 此时就不再是 1v1(一对一) 了, 而是有可能出现 1vn (一对多)\n",
    "    * 下图就是两个 名字 映射到 哈希表上同一个index 的情况\n",
    "![jupyter](https://img-blog.csdnimg.cn/2021010423494884.png)\n",
    "\n",
    "\n",
    "\n",
    "### 哈西碰撞的解决方法\n",
    "* 拉链法\n",
    "    * 发生冲突的元素都被存储在链表中\n",
    "        * 拉链法就是要选择适当的哈希表的大小，这样既不会因为数组空值而浪费大量内存，也不会因为链表太长而在查找上浪费太多时间。\n",
    "    ![jupyter](https://img-blog.csdnimg.cn/20210104235015226.png)\n",
    "\n",
    "\n",
    "\n",
    "* 线性探测法\n",
    "    * 一定要保证 tableSize 大于 dataSize 。 我们需要 **依靠哈希表中的空位** 来解决碰撞问题。\n",
    "        * 例如冲突的位置，放了小李，那么就向下找一个空位放置小王的信息。所以要求tableSize一定要大于dataSize ，要不然哈希表上就没有空置的位置来存放 冲突的数据了\n",
    "        ![jupyter](https://img-blog.csdnimg.cn/20210104235109950.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见的三种哈西结构\n",
    "* 数组\n",
    "* set (集合)\n",
    "* map (映射)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对于 C++ 而言\n",
    "* set 和 map 都分别具有3种数据结构 \n",
    "    * set, multiset, unordered_set\n",
    "    * map, multimap, unordered_map\n",
    "\n",
    "## 对于 set 来说 (value 不能重复)\n",
    "| 集合                 | 底层实现 | 是否有序 | 数值是否可以重复 | 能否更改数值 | 查询效率     | 增删效率     |\n",
    "|--------------------|------|------|----------|--------|----------|----------|\n",
    "| std::set           | 红黑树  | 有序   | 否        | 否      | O(log n) | O(log n) |\n",
    "| std::multiset      | 红黑树  | 有序   | 是        | 否      | O(logn)  | O(logn)  |\n",
    "| std::unordered_set | 哈希表  | 无序   | 否        | 否      | O(1)     | O(1)     |\n",
    "\n",
    "\n",
    "* std::unordered_set 底层实现为哈希表\n",
    "* std::set 和 std::multiset 的底层实现是红黑树\n",
    "    * 红黑树是一种平衡二叉搜索树，所以 key 值是有**序的**，但key不可以修改，改动key值会导致整棵树的错乱，所以只能删除和增加。\n",
    "\n",
    "* 当我们要使用 **集合** 来解决哈希问题的时候，优先使用unordered_set，因为它的查询和增删效率是最优的，如果需要集合是 **有序** 的，那么就用set，如果要求 **不仅有序还要有重复数据** 的话，那么就用multiset。\n",
    "\n",
    "## 对于 map 来说 (value 可重复):\n",
    "| 映射                 | 底层实现 | 是否有序  | 数值是否可以重复 | 能否更改数值  | 查询效率     | 增删效率     |\n",
    "|--------------------|------|-------|----------|---------|----------|----------|\n",
    "| std::map           | 红黑树  | key有序 | key不可重复  | key不可修改 | O(logn)  | O(logn)  |\n",
    "| std::multimap      | 红黑树  | key有序 | key可重复   | key不可修改 | O(log n) | O(log n) |\n",
    "| std::unordered_map | 哈希表  | key无序 | key不可重复  | key不可修改 | O(1)     | O(1)     |\n",
    "\n",
    "* std::unordered_map 底层实现为哈希表\n",
    "* std::map 和std::multimap 的底层实现是红黑树\n",
    "    * 因此, std::map 和std::multimap 的**key也是有序**的（这个问题也经常作为面试题，考察对语言容器底层的理解）。\n",
    "\n",
    "* 在 map 是一个key value 的数据结构，map中，对key是有限制，对value没有限制的，因为key的存储方式使用红黑树实现的。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 虽然std::set、std::multiset 的底层实现是红黑树，不是哈希表，std::set、std::multiset 使用红黑树来索引和存储，不过给我们的使用方式，还是哈希法的使用方式，即key和value。所以使用这些数据结构来解决映射问题的方法，我们依然称之为哈希法。 map也是一样的道理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些C++的经典书籍上 例如STL源码剖析，说到了hash_set hash_map，这个与unordered_set，unordered_map又有什么关系呢？\n",
    "\n",
    "* 实际上功能都是一样一样的， 但是unordered_set在C++11的时候被引入标准库了，而hash_set并没有，所以建议还是使用unordered_set比较好，这就好比一个是官方认证的，hash_set，hash_map 是C++11标准之前民间高手自发造的轮子。\n",
    "\n",
    "![jupyter](https://img-blog.csdnimg.cn/20210104235134572.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关题目\n",
    "* 242.有效的字母异位词\n",
    "* 349 两个数组的交集\n",
    "* 202 快乐数\n",
    "* 1 两数之和\n",
    "* 454 四数相加Ⅱ\n",
    "* 383 赎金信\n",
    "* 15 三数之和 -- 哈希表法容易超时, 用双指针法进行优化\n",
    "* 18 四数之和 -- 哈希表法容易超时, 用双指针法进行优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 针对哈西法求解的题目:\n",
    "* map 是哈希表里的'万能'数据结构, 但是由于它需要使用哈西函数进行一次 key 到 hashCode 的转换, 所以对于非常庞大的数据量下, 它的效率其实很低\n",
    "    * 因此, 在解题时我们应该首先考虑 数组 \n",
    "        * 数组适合存储区域一开始就能估算好的场景, 比如统计 26个字母出现的次数 这样的题目, 一开始就把 a-z 这26 个字母映射成数组的 index, 就非常快\n",
    "        * 数组不适合 存储区域非常不固定的场景, 空间开辟太多容易造成浪费, 空间开辟小了容易err\n",
    "    * 然后考虑 set\n",
    "        * set 是为了解决 数组 要求存储空间固定的问题的\n",
    "        * 但是 set 无法存储 key:value 这种需要同时存储配对信息的场景\n",
    "    * 最后再考虑 map \n",
    "        * map 几乎是万能, 能用 数组 和 set 的场景它都能用, 但是代码执行效率就不好说了\n",
    "            * 比如三数之和, 四数之和这种题目, 因为要存储的 信息 较多, set 不能用, 存储空间不固定, 所以数组也不能用, 而用 map 的话由于效率太低, 直接超时..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 哈希表类型的题目如果超时, 就考虑使用双指针法进行优化\n",
    "* 双指针法的目标在这里通常是把时间复杂度降一个数量级 (因为两个指针分别指向了两个要遍历的元素), 如 $O(n^3)$ 降低为 $O(n^2)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "901177bed68d7d4fbadb3177e755ced472e86ce1c03620f2ee52eeaae03fd4ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
