{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hackerrank.com/challenges/stitch-the-torn-wiki/problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Format\n",
    "\n",
    "An Integer N on the first line. This is followed by 2N+1 lines.\n",
    "\n",
    "Text fragments (numbered 1 to N) from Set A, each on a new line (so a total of N lines).\n",
    "\n",
    "A separator with five asterisk marks \"*\" which indicates the end of Set A and beginning of Set B.\n",
    "\n",
    "Text fragments (numbered 1 to N) from Set B, each on a new line (so a total of N lines).\n",
    "\n",
    "## Output Format\n",
    "\n",
    "N lines, each containing one integer.\n",
    "\n",
    "The i-th line should contain an integer j such that the i-th element of Set A and the j-th element of Set B are a pair, i.e., both originally came from the same block of text/Wikipedia article.\n",
    "\n",
    "## Constraints\n",
    "\n",
    "1 <= N <= 100\n",
    "\n",
    "No text fragment will have more than 10000 characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Input\n",
    "(Please note that the real inputs used will be much longer, and generated with text blocks with 500-1000 words. This is for explanatory purposes only)\n",
    "\n",
    "```txt\n",
    "3\n",
    "Delhi (also known as the National Capital Territory of India) is a metropolitan region in India that includes the national capital city, New Delhi. With a population of 22 million in 2011, it is the world's second most populous city and the largest city in India in terms of area. The NCT and its urban region have been given the special status of National Capital Region (NCR) under the Constitution of India's 69th amendment act of 1991. The NCR includes the neighbouring cities of Baghpat, Gurgaon, Sonepat, Faridabad, Ghaziabad, Noida, Greater Noida and other nearby towns, and has nearly 22.2 million residents.    \n",
    "Seattle is a coastal seaport city and the seat of King County, in the U.S. state of Washington. With an estimated 634,535 residents as of 2012, Seattle is the largest city in the Pacific Northwest region of North America and one of the fastest-growing cities in the United States. The Seattle metropolitan area of around 4 million inhabitants is the 15th largest metropolitan area in the nation.[6] The city is situated on a narrow isthmus between Puget Sound (an inlet of the Pacific Ocean) and Lake Washington, about 100 miles (160 km) south of the Canada–United States border. A major gateway for trade with Asia, Seattle is the 8th largest port in the United States and 9th largest in North America in terms of container handling.  \n",
    "Martin Luther OSA (10 November 1483 – 18 February 1546) was a German monk, Catholic priest, professor of theology and seminal figure of a reform movement in 16th century Christianity, subsequently known as the Protestant Reformation.[1] He strongly disputed the claim that freedom from God's punishment for sin could be purchased with money. He confronted indulgence salesman Johann Tetzel, a Dominican friar, with his Ninety-Five Theses in 1517. His refusal to retract all of his writings at the demand of Pope Leo X in 1520 and the Holy Roman Emperor Charles V at the Diet of Worms in 1521 resulted in his excommunication by the Pope and condemnation as an outlaw by the Emperor.\n",
    "*****  \n",
    "The Seattle area had been inhabited by Native Americans for at least 4,000 years before the first permanent European settlers. Arthur A. Denny and his group of travelers, subsequently known as the Denny Party, arrived at Alki Point on November 13, 1851. The settlement was moved to its current site and named \"Seattle\" in 1853, after Chief Si'ahl of the local Duwamish and Suquamish tribes.  \n",
    "Although technically a federally administered union territory, the political administration of the NCT of Delhi today more closely resembles that of a state of India, with its own legislature, high court and an executive council of ministers headed by a Chief Minister. New Delhi is jointly administered by the federal government of India and the local government of Delhi, and is the capital of the NCT of Delhi.  \n",
    "Luther taught that salvation and subsequently eternity in heaven is not earned by good deeds but is received only as a free gift of God's grace through faith in Jesus Christ as redeemer from sin and subsequently eternity in hell. His theology challenged the authority of the Pope of the Roman Catholic Church by teaching that the Bible is the only source of divinely revealed knowledge from God and opposed sacerdotalism by considering all baptized Christians to be a holy priesthood. Those who identify with these, and all of Luther's wider teachings, are called Lutherans.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Output\n",
    "\n",
    "2  \n",
    "1  \n",
    "3  \n",
    "\n",
    "\n",
    "### Explanation\n",
    "```txt\n",
    "The first, second and third text fragment of Set A are about Delhi, Seattle and Martin Luther respectively.\n",
    "In set B, the paragraph on Delhi, is the second text fragment.\n",
    "The paragraph on Seattle is the first text fragment in Set B.\n",
    "The paragraph on Martin Luther Kind is the third text fragment in Set B.\n",
    "So, the expected output is 2, 1, 3 respectively.\n",
    "```\n",
    "\n",
    "## Scoring\n",
    "```txt\n",
    "A sample test case with twenty paragraphs is provided to you when you Compile and Test.\n",
    "Extensive training data is not required for this challenge. The weightage for a test case will be proportional to the number of tests (Articles) which it contains. This works out to a ratio of 1:2 (Sample Test: Hidden Test).\n",
    "Score = M * (C)/N Where M is the Maximum Score for the test case.\n",
    "C = Number of correct answers in your output.\n",
    "N = Total number of Wikipedia Articles (which were split into 2N fragments and divided into Set A and Set B respectively).\n",
    "```\n",
    "\n",
    "Note:\n",
    "    Submissions will be disqualified if it is evident that the code has been written in such a way that the sample test case answers are hard-coded, or similar approaches, where the answer is not computed, but arrived at by trying to ensure the code matches the sample answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1 (score: 92.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_matching_fragments(N, fragments):\n",
    "    set_a = fragments[:N]\n",
    "    set_b = fragments[N+1:]\n",
    "    \n",
    "    # 文本预处理\n",
    "    vectorizer = CountVectorizer().fit(set_a + set_b)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    # 计算TF-IDF\n",
    "    tfidf_a = tfidf_transformer.fit_transform(vectorizer.transform(set_a))\n",
    "    tfidf_b = tfidf_transformer.transform(vectorizer.transform(set_b))\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarities = cosine_similarity(tfidf_a, tfidf_b)\n",
    "    \n",
    "    # 匹配片段\n",
    "    result = []\n",
    "    for i in range(N):\n",
    "        most_similar_index = np.argmax(similarities[i])\n",
    "        result.append(most_similar_index + 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 读取输入\n",
    "N = int(input())\n",
    "fragments = [input().strip() for _ in range(2 * N + 1)]\n",
    "\n",
    "# 找到匹配的片段\n",
    "result = find_matching_fragments(N, fragments)\n",
    "\n",
    "# 输出结果\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 2 (score: 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 将文本分割成单词，并去除特殊字符\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return words\n",
    "\n",
    "def create_word_count_dict(texts):\n",
    "    # 创建词频字典\n",
    "    word_count_dicts = [Counter(preprocess_text(text)) for text in texts]\n",
    "    return word_count_dicts\n",
    "\n",
    "def create_global_word_count_dict(word_count_dicts):\n",
    "    # 创建全局词频字典\n",
    "    global_word_count = Counter()\n",
    "    for word_count in word_count_dicts:\n",
    "        global_word_count.update(word_count)\n",
    "    return global_word_count\n",
    "\n",
    "def normalize_word_count_dict(word_count_dict, global_word_count):\n",
    "    # 归一化词频字典\n",
    "    total_words = sum(global_word_count.values())\n",
    "    normalized_dict = {word: count / global_word_count[word] for word, count in word_count_dict.items()}\n",
    "    return normalized_dict\n",
    "\n",
    "def dict_to_vector(word_count_dict, global_word_count):\n",
    "    # 将词频字典转换为向量\n",
    "    vector = np.zeros(len(global_word_count))\n",
    "    for i, word in enumerate(global_word_count):\n",
    "        if word in word_count_dict:\n",
    "            vector[i] = word_count_dict[word]\n",
    "    return vector\n",
    "\n",
    "def find_matching_fragments(N, fragments):\n",
    "    set_a = fragments[:N]\n",
    "    set_b = fragments[N+1:]\n",
    "    \n",
    "    # 创建词频字典\n",
    "    word_count_dicts_a = create_word_count_dict(set_a)\n",
    "    word_count_dicts_b = create_word_count_dict(set_b)\n",
    "    \n",
    "    # 创建全局词频字典\n",
    "    global_word_count = create_global_word_count_dict(word_count_dicts_a + word_count_dicts_b)\n",
    "    \n",
    "    # 归一化词频字典\n",
    "    normalized_dicts_a = [normalize_word_count_dict(word_count, global_word_count) for word_count in word_count_dicts_a]\n",
    "    normalized_dicts_b = [normalize_word_count_dict(word_count, global_word_count) for word_count in word_count_dicts_b]\n",
    "    \n",
    "    # 将词频字典转换为向量\n",
    "    vectors_a = np.array([dict_to_vector(word_count, global_word_count) for word_count in normalized_dicts_a])\n",
    "    vectors_b = np.array([dict_to_vector(word_count, global_word_count) for word_count in normalized_dicts_b])\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarity_matrix = cosine_similarity(vectors_a, vectors_b)\n",
    "    \n",
    "    # 找到最佳匹配\n",
    "    row_ind, col_ind = linear_sum_assignment(-similarity_matrix)    # 本质上就是\"匈牙利算法\"; \n",
    "                                                                    # 线性分配问题的目标是找到一种分配方式，使得总成本最小化。因为我们需要找相似度最高的, 刚好与定义相反发, 所以取负值\n",
    "                                                                    # row_ind[i] 表示集合 A 中的第 i 个片段，col_ind[i] 表示集合 B 中与之匹配的片段。\n",
    "    \n",
    "    return col_ind + 1\n",
    "\n",
    "# 读取输入\n",
    "N = int(input())\n",
    "fragments = [input().strip() for _ in range(2 * N + 1)]\n",
    "\n",
    "# 找到匹配的片段\n",
    "result = find_matching_fragments(N, fragments)\n",
    "\n",
    "# 输出结果\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匈牙利算法\n",
    "\n",
    "匈牙利算法（Hungarian Algorithm），也称为Kuhn-Munkres算法，是一种用于解决二分图匹配问题的组合优化算法。它可以在多项式时间内找到最大权匹配或最小权匹配，常用于解决分配问题（Assignment Problem）。以下是对匈牙利算法的详细解释：\n",
    "\n",
    "## 问题背景\n",
    "在分配问题中，我们有两组元素，例如工人和任务，每个工人完成每个任务都有一个成本或收益。目标是找到一种分配方式，使得总成本最小或总收益最大。\n",
    "\n",
    "## 算法步骤\n",
    "匈牙利算法的核心思想是通过一系列矩阵操作来简化问题，最终找到最优匹配。以下是算法的主要步骤：\n",
    "\n",
    "## 构建初始成本矩阵：\n",
    "\n",
    "将问题表示为一个N x N的成本矩阵，其中每个元素表示一个工人完成一个任务的成本。\n",
    "\n",
    "## 行操作：\n",
    "\n",
    "对每一行，减去该行的最小值，使得每一行至少有一个零。\n",
    "\n",
    "## 列操作：\n",
    "\n",
    "对每一列，减去该列的最小值，使得每一列至少有一个零。\n",
    "\n",
    "## 覆盖零：\n",
    "\n",
    "使用尽可能少的水平线和垂直线覆盖所有的零。如果覆盖线的数量等于矩阵的维度，则找到最优匹配；否则，继续下一步。\n",
    "\n",
    "## 调整矩阵：\n",
    "\n",
    "找到未被覆盖的最小元素，将其从未覆盖的元素中**减去**，**并将其加到被覆盖两次的元素上**。重复步骤4和5，直到找到最优匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子:\n",
    "假设有如下成本矩阵:\n",
    "```txt\n",
    "  1  2  3\n",
    "1 4  1  3\n",
    "2 2  0  5\n",
    "3 3  2  2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤1：行操作\n",
    "\n",
    "对每一行减去该行的最小值\n",
    "\n",
    "```txt\n",
    "  1  2  3\n",
    "1 3  0  2\n",
    "2 2  0  5\n",
    "3 1  0  0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤2：列操作\n",
    "对每一列减去该列的最小值：\n",
    "\n",
    "```txt\n",
    "  1  2  3\n",
    "1 2  0  2\n",
    "2 1  0  5\n",
    "3 0  0  0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤3：覆盖零\n",
    "\n",
    "使用尽可能少的线覆盖所有的零：\n",
    "```txt\n",
    "  1  2  3\n",
    "1 2  0  2\n",
    "2 1  0  5\n",
    "3 0  0  0\n",
    "```\n",
    "可以用两条线覆盖所有的零（例如，覆盖第2列和第3行）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤4：调整矩阵\n",
    "\n",
    "找到未被覆盖的最小元素（1），调整矩阵：\n",
    "\n",
    "```txt\n",
    "  1  2  3\n",
    "1 1  0  1\n",
    "2 0  0  4\n",
    "3 0  0  0\n",
    "```\n",
    "\n",
    "重复覆盖零和调整矩阵的步骤，直到找到最优匹配。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用场景\n",
    "\n",
    "匈牙利算法广泛应用于各种分配问题，例如：\n",
    "\n",
    "- 任务分配：将任务分配给工人，使得总成本最小。\n",
    "- 学生宿舍分配：将学生分配到宿舍，使得总满意度最大。\n",
    "- 图像处理：在多目标跟踪中，将检测到的目标分配给跟踪器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
